# ğŸš€ Week 7 â€“ Section 1

# ğŸ³ Docker CI/CD (Offline Deployment + Rollback)

ğŸ‘¨â€ğŸ’» **Nanosoft Technology â€“ DevOps Internship**

ğŸ³ Environment: Jenkins + Docker + Target VM (192.168.56.111)

---

## ğŸ“˜ Objective

The objective of this task was to implement a **production-style Docker CI/CD pipeline** using Jenkins that performs:

- Docker image build
- Commit-hash tagging
- Security scanning (Trivy)
- Offline image packaging (TAR)
- Secure transfer via SCP
- Remote deployment
- Health check validation
- Automatic rollback on failure

This simulates a **real enterprise environment where Docker Hub push is restricted**.

---

# ğŸ§© Task â€“ Docker + CI/CD Pipeline Implementation

---

## ğŸ”„ Stage 1 â€“ Prepare App + Git Commit

- Captured latest commit hash
- Stored commit ID for image tagging
- Ensured reproducible builds

Pipeline Stage:

```
Prepare App + Git Commit
```

---

## ğŸ— Stage 2 â€“ Build Docker Image (Tag = Commit Hash)

```bash
docker build -t week7-demo:<commit-hash> .
```

âœ” Image versioned using commit hash  
âœ” Ensures traceable deployments

---

## ğŸ” Stage 3 â€“ Scan Image (Trivy)

```bash
trivy image week7-demo:<commit-hash>
```

Verified:

- Vulnerability scan completed
- No critical security issues blocking deployment

---

## ğŸ“¦ Stage 4 â€“ Package Image as TAR (Offline Push)

Instead of pushing to Docker Hub:

```bash
docker save week7-demo:<commit-hash> -o week7-demo-<commit>.tar.gz
```

âœ” Enables air-gapped deployment  
âœ” Secure internal infrastructure simulation

---

## ğŸ“¤ Stage 5 â€“ Upload TAR to Deployment VM (SCP)

```bash
scp week7-demo-<commit>.tar.gz user@192.168.56.111:/opt/deploy/
```

Verified:

- Secure transfer successful
- Target VM ready for image loading

---

## ğŸš€ Stage 6 â€“ Deploy on Target + Health Check

On Target VM:

```bash
docker load -i week7-demo-<commit>.tar.gz
docker run -d -p 3000:3000 week7-demo:<commit>
```

Health check validation:

```bash
curl http://localhost:3000
```

Observed:

```
Health HTTP code: 200
DEPLOY OK: <commit>
```

âœ” Successful deployment  
âœ” Application reachable  
âœ” Port 3000 exposed

---

## ğŸ” Rollback Strategy (If Deployment Fails)

If health check fails:

```bash
docker run -d -p 3000:3000 week7-demo:<previous-commit>
```

âœ” Automatic recovery  
âœ” Version-controlled rollback  
âœ” Zero downtime simulation

---

## ğŸ§  Concepts Learned

- CI/CD automation with Jenkins
- Docker image versioning strategy
- Security scanning inside pipeline
- Offline Docker deployment model
- SCP-based production deployment
- Health check validation
- Rollback mechanism design
- Production-grade deployment workflow

---

# ğŸ“¸ Snapshots

Below are screenshots demonstrating successful pipeline execution and deployment verification.

---

## ğŸ–¼ Pipeline Overview

![Docker-CICD-1](snapshots/Docker%20CICD/1.png)

---

## ğŸ–¼ Upload TAR to Deployment VM (SCP Stage)

![Docker-CICD-2](snapshots/Docker%20CICD/2.png)

---

## ğŸ–¼ Deploy on Target + Health Check Success

![Docker-CICD-3](snapshots/Docker%20CICD/3.png)

---

# ğŸš€ Week 7 â€“ Section 2

# ğŸ³ Docker Compose â€“ Multi-Container Application

---

ğŸ‘¨â€ğŸ’» **Nanosoft Technology â€“ DevOps Internship**  
ğŸ³ Environment: Docker Compose (Bridge Network + Named Volumes)

---

## ğŸ“˜ Objective

The objective of this task was to design and deploy a **multi-container production-style application** using Docker Compose that includes:

- Backend APIs (Node + Java)
- Frontend client
- Nginx reverse proxy
- MySQL database
- MongoDB database
- Named volumes for persistence
- Custom bridge network
- Environment variable configuration
- Inter-container communication

This simulates a real microservices-based architecture.

---

# ğŸ§© Task â€“ Multi-Container Architecture Using Docker Compose

---

## ğŸ— Application Stack

The application consists of:

| Service | Technology   | Purpose         |
| ------- | ------------ | --------------- |
| client  | Frontend App | UI Layer        |
| api     | Node.js API  | Backend Service |
| webapi  | Java API     | Backend Service |
| emartdb | MySQL 8.0    | Relational DB   |
| emongo  | MongoDB 4    | NoSQL DB        |
| nginx   | Nginx        | Reverse Proxy   |

---

## ğŸ“‚ Project Structure

Observed in:

```bash
ls -la
```

Includes:

- Dockerfile
- docker-compose.yml
- .env
- client/
- nodeapi/
- javaapi/
- nginx/
- Named volume configuration

---

## ğŸŒ Custom Network Configuration

```yaml
networks:
  emart_net:
    driver: bridge
```

âœ” All services connected to internal bridge network  
âœ” Secure inter-service communication  
âœ” Isolation from host network

---

## ğŸ’¾ Named Volumes (Persistent Storage)

```yaml
volumes:
  mongo_data:
  mysql_data:
```

Used as:

```yaml
mongo_data:/data/db
mysql_data:/var/lib/mysql
```

âœ” Database data persists after container restart  
âœ” Production-grade storage handling

---

## ğŸ” Environment Variables (.env)

Configured:

```env
MYSQL_ROOT_PASSWORD=****
MYSQL_DATABASE=books
MONGO_INITDB_DATABASE=****
```

Used inside docker-compose.yml:

```yaml
environment:
  - MYSQL_ROOT_PASSWORD=${MYSQL_ROOT_PASSWORD}
```

âœ” Secure configuration management  
âœ” No hardcoded credentials

---

## â–¶ Build & Start Containers

```bash
docker compose up -d --build
```

Observed:

- Images built successfully
- Network created
- Volumes created
- Containers started

---

## ğŸ“Š Container Verification

```bash
docker compose ps
```

Verified:

- api â†’ Running
- client â†’ Running
- webapi â†’ Running
- emartdb â†’ Running
- emongo â†’ Running
- nginx â†’ Running

---

## ğŸ—„ Database Verification (MySQL)

```bash
docker compose exec emartdb sh -c \
'mysql -uroot -p"$MYSQL_ROOT_PASSWORD" -e "SHOW DATABASES;"'
```

Output confirmed:

- books
- information_schema
- mysql
- performance_schema
- sys

âœ” Database successfully initialized  
âœ” Environment variables applied

---

## ğŸŒ Application Access via Nginx

```bash
curl -I http://localhost
```

Response:

```
HTTP/1.1 200 OK
Server: nginx/1.29.4
```

âœ” Reverse proxy working  
âœ” Multi-container routing successful  
âœ” Port 80 exposed

---

# ğŸ§  Concepts Learned

- Docker Compose orchestration
- Multi-container microservices design
- Bridge networking
- Named volumes & persistence
- Environment variable injection
- Service dependency handling
- Reverse proxy configuration
- Container-to-container communication

---

# ğŸ“¸ Snapshots

---

## ğŸ–¼ Database Verification

![Compose-1](snapshots/Docker%20Compose%20%E2%80%93%20Multi-Container%20Application/1.png)

---

## ğŸ–¼ Project Structure

![Compose-2](snapshots/Docker%20Compose%20%E2%80%93%20Multi-Container%20Application/2.png)

---

## ğŸ–¼ Container Status (docker compose ps)

![Compose-3](snapshots/Docker%20Compose%20%E2%80%93%20Multi-Container%20Application/3.png)

---

## ğŸ–¼ docker-compose.yml Configuration

![Compose-4](snapshots/Docker%20Compose%20%E2%80%93%20Multi-Container%20Application/4.png)

---

# âœ… Outcome

Successfully deployed a **production-style multi-container architecture** using Docker Compose featuring:

âœ” Backend services  
âœ” Frontend service  
âœ” Reverse proxy  
âœ” MySQL & MongoDB  
âœ” Named volumes  
âœ” Custom bridge network  
âœ” Environment configuration

This demonstrates real-world containerized microservices deployment.

---

# ğŸš€ Week 7 â€“ Section 3

# ğŸ³ Docker Deep Dive (Networking, Isolation, Overlay, tcpdump)

---

ğŸ‘¨â€ğŸ’» **Nanosoft Technology â€“ DevOps Internship**  
ğŸ³ Environment: Docker Networking (Bridge, Host, Overlay, Swarm)

---

## ğŸ“˜ Objective

The objective of this task was to deeply understand Docker networking internals by:

- Creating multiple custom bridge networks
- Testing isolation between networks
- Fixing connectivity manually
- Demonstrating host network behavior
- Simulating overlay networks (Swarm mode)
- Capturing real traffic using tcpdump
- Analyzing ICMP packets from captured pcap

This task focused on **real packet-level understanding of Docker networking**.

---

# ğŸ§© Task 1 â€“ Create Custom Bridge Networks

---

## ğŸ”¹ Create Networks

```bash
docker network create mybridge1
docker network create mybridge2
```

Verify:

```bash
docker network ls
```

Observed:

- mybridge1 (bridge)
- mybridge2 (bridge)
- host
- overlay networks (Swarm)

---

# ğŸ§© Task 2 â€“ Network Isolation Test

---

## ğŸ”¹ Run Containers on Different Networks

```bash
docker run -d --name c1 --network mybridge1 alpine sleep 1d
docker run -d --name c2 --network mybridge2 alpine sleep 1d
```

IP Allocation:

- c1 â†’ 172.18.0.2
- c2 â†’ 172.19.0.2

---

## ğŸ”¹ Ping Test (Expected Failure)

```bash
docker exec c1 ping -c 2 <c2-ip>
```

Result:

```
Ping exit code: 1 (expected non-zero)
```

âœ” Containers on different bridge networks cannot communicate  
âœ” Network isolation confirmed

---

# ğŸ§© Task 3 â€“ Fix Connectivity Manually

---

## ğŸ”¹ Connect c2 to mybridge1

```bash
docker network connect mybridge1 c2
```

New IP assigned:

- c2 on mybridge1 â†’ 172.18.0.3

---

## ğŸ”¹ Ping Test After Fix

```bash
docker exec c1 ping -c 2 172.18.0.3
```

Result:

```
0% packet loss
```

âœ” Connectivity restored  
âœ” Multi-network attachment demonstrated

---

# ğŸ§© Task 4 â€“ Host Network Mode Demo

---

## ğŸ”¹ Run Container Using Host Network

```bash
docker run -d --name hostweb --network host python:3.11-alpine \
sh -c "python -m http.server 8080"
```

Test:

```bash
curl http://127.0.0.1:8080
```

âœ” Direct host binding  
âœ” No container network isolation  
âœ” Shares host networking stack

---

# ğŸ§© Task 5 â€“ Overlay Network (Swarm Mode)

---

## ğŸ”¹ Create Swarm Services

```bash
docker service create --name s1 --replicas 2 alpine sleep 1000
docker service create --name s2 --replicas 2 alpine sleep 1000
```

Verify:

```bash
docker service ps s1
docker service ps s2
```

---

## ğŸ”¹ Overlay Communication Test

```bash
docker run --rm --network myoverlay1 alpine \
sh -c "ping -c 2 s1 && ping -c 2 s2"
```

Observed:

```
0% packet loss
```

âœ” Overlay networking works across replicas  
âœ” Service discovery functioning

---

# ğŸ§© Task 6 â€“ Capture Traffic with tcpdump

---

## ğŸ”¹ Capture ICMP Traffic

```bash
docker exec c1 ping -c 4 c2
```

Traffic captured:

```
/home/vagrant/week7-networking/results/icmp_capture.pcap
```

---

## ğŸ”¹ Inspect pcap File

```bash
tcpdump -r icmp_capture.pcap
```

Observed:

```
ICMP echo request
ICMP echo reply
```

âœ” Packet-level verification  
âœ” Bridge interface capture successful

---

# ğŸ§  Concepts Learned

- Docker bridge networking
- Network namespace isolation
- Multi-network container attachment
- Host networking behavior
- Swarm overlay networking
- Service discovery in Swarm
- Packet capture using tcpdump
- ICMP traffic analysis
- Linux bridge interfaces
- Container networking internals

---

# ğŸ“¸ Snapshots

All images stored under:

```
Week 7/images/
```

---

## ğŸ–¼ Network Isolation Test

![DeepDive-1](snapshots/Docker%20Deep%20Dive/1.png)

---

## ğŸ–¼ Connectivity Fix

![DeepDive-2](snapshots/Docker%20Deep%20Dive/2.png)

---

## ğŸ–¼ Host Network Demo

![DeepDive-3](snapshots/Docker%20Deep%20Dive/3.png)

---

## ğŸ–¼ Swarm Overlay Networking

![DeepDive-4](snapshots/Docker%20Deep%20Dive/4.png)

---

## ğŸ–¼ tcpdump Capture

![DeepDive-5](snapshots/Docker%20Deep%20Dive/5.png)

---

## ğŸ–¼ pcap Analysis

![DeepDive-6](snapshots/Docker%20Deep%20Dive/6.png)

---

# âœ… Outcome

Successfully demonstrated:

âœ” Network isolation  
âœ” Manual connectivity control  
âœ” Host vs Bridge comparison  
âœ” Overlay networking via Swarm  
âœ” Real packet capture & inspection

This section proves deep understanding of Docker networking at infrastructure level.

---

# ğŸš€ Week 7 â€“ Section 4

# ğŸ“Š Docker Logs & Monitoring (Grafana + Loki + Promtail)

ğŸ‘¨â€ğŸ’» **Nanosoft Technology â€“ DevOps Internship**

ğŸ³ Environment: Docker Logging, Loki Stack, Log Rotation

---

## ğŸ“˜ Objective

The objective of this task was to:

- Understand Docker logging drivers
- Configure log rotation (json-file driver)
- Deploy Loki log aggregation stack
- Integrate Promtail to ship container logs
- Visualize logs in Grafana
- Verify logs locally and inside Loki
- Inspect container log paths manually

This task focused on **centralized logging and observability in Docker environments**.

---

# ğŸ§© Task 1 â€“ Verify Docker Logging Driver

---

## ğŸ”¹ Check Current Logging Driver

```bash
docker info | grep -A5 "Logging Driver"
```

Observed:

```
Logging Driver: json-file
```

âœ” Docker uses `json-file` by default
âœ” Logs stored under `/var/lib/docker/containers/...`

---

# ğŸ§© Task 2 â€“ Configure Log Rotation

---

## ğŸ”¹ Configure daemon.json

```json
{
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "5m",
    "max-file": "3"
  }
}
```

This ensures:

- Each log file max 5MB
- Only 3 rotated files kept
- Prevents disk exhaustion

After configuration:

```bash
sudo systemctl restart docker
```

âœ” Log rotation enabled

---

# ğŸ§© Task 3 â€“ Deploy Loki Stack

Services started:

- grafana
- loki
- promtail
- test log containers (log-app, rotate-test)

Verify:

```bash
docker ps
```

Observed:

- Grafana â†’ 3000
- Loki â†’ 3100
- Promtail running
- log-app generating logs

---

# ğŸ§© Task 4 â€“ Verify Logs Locally

---

## ğŸ”¹ App Generating Logs

Example:

```
INFO Hello from log-app line=7
INFO Hello from log-app line=8
...
```

Logs verified using:

```bash
docker logs log-app
```

âœ” Logs generated successfully
âœ” Stored locally using json-file

---

# ğŸ§© Task 5 â€“ Inspect Container Log File Path

---

```bash
docker inspect rotate-test --format '{{.LogPath}}'
```

Observed:

```
/var/lib/docker/containers/<container-id>/<container-id>-json.log
```

âœ” Confirmed physical log storage location

---

# ğŸ§© Task 6 â€“ Access Grafana

---

Grafana URL:

```
http://localhost:3000
```

Default credentials:

```
admin / admin
```

âœ” Grafana login successful

---

# ğŸ§© Task 7 â€“ Query Logs in Loki

---

Inside Grafana â†’ Explore â†’ Loki

Query used:

```
{job="docker"}
```

Observed:

- Logs visible from containers
- Log volume graph displayed
- Query inspector stats visible

Example Stats:

- Total request time: 426 ms
- Lines processed/sec: 34712
- Total bytes processed: 3.05 MB

âœ” Centralized logging working
âœ” Loki receiving logs from Promtail

---

# ğŸ§  Architecture Flow

```
Docker Containers
        â†“
json-file logs
        â†“
Promtail
        â†“
Loki
        â†“
Grafana
```

This demonstrates a production-grade log aggregation pipeline.

---

# ğŸ“¸ Snapshots

## ğŸ–¼ Loki Query in Grafana

![Logs-1](snapshots/Docker%20Logs%20%26%20Monitoring/1.png)

---

## ğŸ–¼ Docker Logging Driver Verification

![Logs-2](snapshots/Docker%20Logs%20%26%20Monitoring/2.png)

---

## ğŸ–¼ Grafana Login Page

![Logs-3](snapshots/Docker%20Logs%20%26%20Monitoring/3.png)

---

## ğŸ–¼ Running Containers

![Logs-4](snapshots/Docker%20Logs%20%26%20Monitoring/4.png)

---

## ğŸ–¼ Inspect Log Path

![Logs-5](snapshots/Docker%20Logs%20%26%20Monitoring/5.png)

---

# âœ… Outcome

Successfully implemented:

âœ” Docker log rotation
âœ” Centralized logging with Loki
âœ” Log shipping with Promtail
âœ” Log visualization in

# ğŸš€ Week 7 â€“ Section 5

# ğŸ³ Docker Swarm Lab (Multi-Node Cluster)

ğŸ‘¨â€ğŸ’» **Nanosoft Technology â€“ DevOps Internship**

ğŸ³ Environment: 1 Manager + 2 Worker Nodes (Vagrant-based Swarm Cluster)

---

## ğŸ“˜ Objective

The objective of this lab was to:

- Initialize a Docker Swarm cluster
- Join multiple worker nodes
- Deploy replicated services
- Verify task distribution across nodes
- Perform rolling updates
- Observe node failure handling
- Understand Swarm orchestration behavior

This task focused on **container orchestration using native Docker Swarm**.

---

# ğŸ§© Task 1 â€“ Initialize Swarm (Manager Node)

---

## ğŸ”¹ On Manager

```bash
docker swarm init --advertise-addr 192.168.56.10
```

Observed:

- Swarm initialized
- Manager node became Leader
- Join token generated

âœ” Cluster created successfully

---

# ğŸ§© Task 2 â€“ Join Worker Nodes

---

## ğŸ”¹ On worker1

```bash
docker swarm join --token <TOKEN> 192.168.56.10:2377
```

Result:

```
This node joined a swarm as a worker.
```

---

## ğŸ”¹ On worker2

```bash
docker swarm join --token <TOKEN> 192.168.56.10:2377
```

âœ” Both workers successfully joined cluster

---

## ğŸ”¹ Verify Nodes (Manager)

```bash
docker node ls
```

Observed:

- manager â†’ Leader
- worker1 â†’ Ready
- worker2 â†’ Ready

âœ” Multi-node cluster operational

---

# ğŸ§© Task 3 â€“ Deploy Replicated Service

---

## ğŸ”¹ Create Nginx Service

```bash
docker service create \
  --name web \
  --replicas 3 \
  -p 8080:80 \
  nginx
```

Observed:

```
overall progress: 3 out of 3 tasks
verify: Service converged
```

âœ” 3 replicas running
âœ” Published port 8080

---

## ğŸ”¹ Verify Service

```bash
docker service ls
```

Result:

```
web   replicated   3/3   nginx:latest   *:8080->80/tcp
```

---

## ğŸ”¹ Inspect Task Placement

```bash
docker service ps web
```

Observed:

- web.1 â†’ worker1
- web.2 â†’ worker2
- web.3 â†’ manager

âœ” Swarm distributed tasks automatically

---

## ğŸ”¹ Access Service

```bash
curl http://localhost:8080
```

Returned:

```
Welcome to nginx!
```

âœ” Routing mesh working
âœ” Service reachable from manager

---

# ğŸ§© Task 4 â€“ Rolling Update

---

## ğŸ”¹ Update Image Version

```bash
docker service update \
  --image nginx:1.25 \
  --update-delay 5s \
  web
```

Observed:

- Tasks replaced one-by-one
- Old containers shutdown
- New version deployed

âœ” Zero downtime rolling update successful

---

# ğŸ§© Task 5 â€“ Node Failure Simulation

---

## ğŸ”¹ Stop worker1 VM

```bash
vagrant halt worker1
```

---

## ğŸ”¹ Verify Node Status

```bash
docker node ls
```

Observed:

- worker1 â†’ Down
- worker2 â†’ Ready
- manager â†’ Leader

âœ” Swarm detected node failure

---

## ğŸ”¹ Verify Service Tasks

```bash
docker service ps web
```

Observed:

- Failed tasks re-scheduled
- Service maintained desired replicas

âœ” Self-healing behavior confirmed

---

# ğŸ§  Key Concepts Learned

- Swarm Manager vs Worker roles
- Leader election
- Service replication
- Routing mesh
- Rolling updates
- Self-healing containers
- Node availability states
- Distributed orchestration

---

# ğŸ“¸ Snapshots

## ğŸ–¼ Swarm Init

![Swarm-1](snapshots/Docker%20Swarm/1.png)

---

## ğŸ–¼ Worker Join

![Swarm-2](snapshots/Docker%20Swarm/2.png)

---

## ğŸ–¼ Node List

![Swarm-3](snapshots/Docker%20Swarm/3.png)

---

## ğŸ–¼ Service Deployment

![Swarm-4](snapshots/Docker%20Swarm/4.png)

---

## ğŸ–¼ Service Tasks Distribution

![Swarm-5](snapshots/Docker%20Swarm/5.png)

---

## ğŸ–¼ Rolling Update

![Swarm-6](snapshots/Docker%20Swarm/6.png)

---

## ğŸ–¼ VM Simulation

![Swarm-7](snapshots/Docker%20Swarm/7.png)

---

## ğŸ–¼ Node Down Detection

![Swarm-8](snapshots/Docker%20Swarm/8.png)

---

# âœ… Outcome

Successfully demonstrated:

âœ” Multi-node Swarm cluster
âœ” Replicated service deployment
âœ” Automatic load distribution
âœ” Rolling updates with zero downtime
âœ” Self-healing after

# ğŸš€ Week 7 â€“ Section 6

# ğŸ³ Docker Volumes & Backup (PostgreSQL Data Protection Lab)

ğŸ‘¨â€ğŸ’» **Nanosoft Technology â€“ DevOps Internship**

ğŸ³ Environment: Docker + PostgreSQL + Named Volumes + Remote Backup Server

---

## ğŸ“˜ Objective

The objective of this lab was to:

- Create and inspect Docker named volumes
- Store PostgreSQL data inside volumes
- Perform database dump backup
- Compress and upload backup to remote server
- Restore database from backup
- Verify data integrity after restore
- Understand persistent storage in containers

This task focused on **data durability and disaster recovery in containerized environments**.

---

# ğŸ§© Task 1 â€“ Verify Docker Volume

---

## ğŸ”¹ List Docker Volumes

```bash
docker volume ls | grep pgdata_week7
```

Observed:

```
local   pgdata_week7
```

âœ” Named volume created successfully

---

## ğŸ”¹ Inspect Volume Mount

```bash
docker inspect pgdb --format '{{ json .Mounts }}' | jq
```

Observed:

```json
{
  "Type": "volume",
  "Name": "pgdata_week7",
  "Source": "/var/lib/docker/volumes/pgdata_week7/_data",
  "Destination": "/var/lib/postgresql/data",
  "Driver": "local",
  "RW": true
}
```

âœ” PostgreSQL data stored inside Docker-managed volume
âœ” Data persisted outside container filesystem

---

# ğŸ§© Task 2 â€“ Database Backup

---

## ğŸ”¹ Generate Database Dump

```bash
docker exec pgdb pg_dump -U week7user week7db > week7db.sql
```

âœ” SQL dump created

---

## ğŸ”¹ Compress Backup

```bash
tar -czf week7db_2026-01-20_09-56-22.tar.gz week7db.sql
```

Backup created:

```
week7db_2026-01-20_09-56-22.tar.gz
```

---

## ğŸ”¹ Upload to Remote Backup Server

```bash
scp week7db_2026-01-20_09-56-22.tar.gz backup-server:/opt/remote-backups/
```

Verified on backup server:

```
/opt/remote-backups/week7db_2026-01-20_09-56-22.tar.gz
```

âœ” Remote backup stored successfully

---

# ğŸ§© Task 3 â€“ Restore Process

---

## ğŸ”¹ Pull Backup

```bash
scp backup-server:/opt/remote-backups/week7db_2026-01-20_09-56-22.tar.gz .
```

---

## ğŸ”¹ Extract Backup

```bash
tar -xzf week7db_2026-01-20_09-56-22.tar.gz
```

---

## ğŸ”¹ Recreate Database

```bash
docker exec -e PGPASSWORD=week7pass pgdb \
psql -U week7user -d week7db -c "DROP DATABASE week7db;"
```

Recreate:

```bash
createdb week7db
```

---

## ğŸ”¹ Import SQL Dump

```bash
docker exec -e PGPASSWORD=week7pass pgdb \
psql -U week7user -d week7db < week7db.sql
```

âœ” Database restored successfully

---

# ğŸ§© Task 4 â€“ Restore Verification

---

## ğŸ”¹ Verify Data

```bash
docker exec -e PGPASSWORD=week7pass pgdb \
psql -U week7user -d week7db \
-c "SELECT * FROM students ORDER BY id;"
```

Observed:

```
1 | Mohsin      | mohsin.khan@example.com
2 | Sarah Ahmed | sarah.ahmed@example.com
3 | Hassan Raza | hassan.raza@example.com
```

âœ” 3 rows restored
âœ” Data integrity confirmed

---

# ğŸ§  Key Concepts Learned

- Docker named volumes
- Volume mount points
- Data persistence outside container lifecycle
- Database dump & restore process
- Remote backup storage
- Disaster recovery workflow
- Separation of data & container

---

# ğŸ“¸ Snapshots

## ğŸ–¼ Volume Inspection

![Volume-1](snapshots/Docker%20Volumes%20%26%20Backup/1.png)

---

## ğŸ–¼ Backup Files Created

![Volume-2](snapshots/Docker%20Volumes%20%26%20Backup/2.png)

---

## ğŸ–¼ Remote Backup Server

![Volume-3](snapshots/Docker%20Volumes%20%26%20Backup/3.png)

---

## ğŸ–¼ Restore Verification

![Volume-4](snapshots/Docker%20Volumes%20%26%20Backup/4.png)

---

## ğŸ–¼ Results Directory

![Volume-5](snapshots/Docker%20Volumes%20%26%20Backup/5.png)

---

## ğŸ–¼ Final Data Check

![Volume-6](snapshots/Docker%20Volumes%20%26%20Backup/6.png)

---

# âœ… Outcome

Successfully demonstrated:

âœ” Persistent storage using Docker volumes
âœ” Safe database backup & compression
âœ” Remote backup upload
âœ” Full restore workflow
âœ” Data verification after restore

This proves strong understanding of **container data management and backup strategy** â€” a critical DevOps skill.

---

# ğŸš€ Week 7 â€“ Section 7

ğŸ³ Production-Ready Dockerfile & Image Optimization

ğŸ‘¨â€ğŸ’» **Nanosoft Technology â€“ DevOps Internship**

ğŸ–¥ Environment: Ubuntu 22.04 LTS (Vagrant VM)
ğŸ³ Tools: Docker, Docker Compose

---

## ğŸ“˜ Objective

The objective of this task was to:

- Optimize Docker images for production use
- Reduce image sizes (before vs after comparison)
- Remove unnecessary build-time dependencies
- Implement efficient Dockerfile practices
- Deploy full multi-service stack using Docker Compose
- Verify application availability via Nginx

This task focused on **real-world container optimization and production deployment readiness.**

---

# ğŸ§© Task 1 â€“ Analyze Existing Images

---

## ğŸ” List Current Images

```bash
docker images | grep emartapp
```

### Observed:

- `emartapp-api:latest` â†’ ~229MB
- `emartapp-client:latest` â†’ ~247MB
- `emartapp-webapi:latest` â†’ ~407MB

Verified images are built and available locally.

---

## ğŸ§  Concepts Learned

- Docker image layers
- How image size affects performance
- Importance of minimal base images
- Layer caching behavior

---

# ğŸ§© Task 2 â€“ Before vs After Image Optimization

---

## ğŸ“¦ WebAPI (Java Service)

```bash
docker images | grep emartapp-webapi
```

### Observed:

- `emartapp-webapi:before` â†’ **1.01GB**
- `emartapp-webapi:after` â†’ **407MB**

âœ” Significant size reduction achieved.

---

## ğŸ“¦ API (Node Service)

```bash
docker images | grep emartapp-api
```

### Observed:

- `emartapp-api:before` â†’ **1.69GB**
- `emartapp-api:after` â†’ **221MB**

âœ” Removed unnecessary build dependencies.
âœ” Optimized runtime container.

---

## ğŸ§  Concepts Learned

- Multi-stage builds
- Separating build image from runtime image
- Reducing attack surface
- Smaller images = faster pull & deploy
- Production Dockerfile best practices

---

# ğŸ§© Task 3 â€“ Deploy Full Application Stack

---

## â–¶ Start Docker Compose Stack

```bash
docker compose up -d
```

---

## ğŸ“Š Verify Running Containers

```bash
docker compose ps
```

### Observed Services:

- api â†’ Port 5000
- client â†’ Port 4200
- webapi â†’ Port 9000
- mysql â†’ Port 3306
- mongo â†’ Port 27017
- nginx â†’ Port 80

All services status: **Up**

---

## ğŸŒ Application Health Check

```bash
curl -I http://localhost
```

### Response:

```
HTTP/1.1 200 OK
Server: nginx/1.29.4
```

âœ” Application accessible via Nginx
âœ” Reverse proxy functioning correctly

---

## ğŸ§  Concepts Learned

- Docker Compose service orchestration
- Port binding
- Service networking (default bridge network)
- Nginx reverse proxy validation
- Full stack deployment testing

---

# ğŸ§  Overall Concepts Learned in This Section

- Production-ready Dockerfile design
- Image size optimization strategies
- Before vs After comparison methodology
- Multi-container deployment
- Runtime verification
- DevOps deployment workflow

---

# ğŸ“¸ Snapshots

Below are screenshots demonstrating image optimization and stack verification.

---

## ğŸ–¼ Image Listing (Latest)

![PR-1](snapshots/Production-Ready%20Dockerfile/1.png)

---

## ğŸ–¼ Docker Compose Running Stack

![PR-2](snapshots/Production-Ready%20Dockerfile/2.png)

---

## ğŸ–¼ WebAPI Before vs After

![PR-3](snapshots/Production-Ready%20Dockerfile/3.png)

---

## ğŸ–¼ API Before vs After

![PR-4](snapshots/Production-Ready%20Dockerfile/4.png)

---

# ğŸš€ Week 7 â€“ Section 8

# ğŸ³ Secure Docker Containers

ğŸ‘¨â€ğŸ’» **Nanosoft Technology â€“ DevOps Internship**

ğŸ–¥ Environment: Ubuntu 22.04 LTS (Vagrant VM)
ğŸ³ Tools: Docker

---

## ğŸ“˜ Objective

The objective of this task was to strengthen container security by:

- Running containers with **read-only filesystem**
- Using **tmpfs mounts** for temporary writable storage
- Applying **memory limits**
- Monitoring container resource usage
- Understanding container isolation & runtime restrictions

This lab focused on implementing **basic container hardening techniques** used in production environments.

---

# ğŸ§© Task 1 â€“ Run Container with Read-Only Filesystem

---

## â–¶ Run Alpine in Read-Only Mode

```bash
docker run --rm --read-only alpine sh -c "touch /testfile"
```

### Observed:

```
touch: /testfile: Read-only file system
```

âœ” Container filesystem is protected
âœ” No file modifications allowed

---

## ğŸ§  Concepts Learned

- `--read-only` flag prevents file writes
- Protects container from runtime tampering
- Useful for immutable production containers

---

# ğŸ§© Task 2 â€“ Allow Temporary Writable Storage (tmpfs)

---

## â–¶ Run with tmpfs Mount

```bash
docker run --rm \
  --read-only \
  --tmpfs /tmp \
  alpine sh -c "touch /tmp/test && echo tmp-ok"
```

### Observed:

```
tmp-ok
```

âœ” `/tmp` writable
âœ” Rest of filesystem remains read-only

---

## ğŸ§  Concepts Learned

- `--tmpfs` creates in-memory filesystem
- Data removed when container stops
- Improves security & performance
- Prevents disk persistence

---

# ğŸ§© Task 3 â€“ Limit Container Memory

---

## â–¶ Run Container with Memory Restriction

```bash
docker run --rm --memory=64m alpine sh -c "echo Memory limited"
```

### Observed:

```
Memory limited
```

âœ” Container memory capped at 64MB

---

## ğŸ§  Concepts Learned

- `--memory` restricts RAM usage
- Prevents resource exhaustion
- Essential for multi-tenant environments
- Protects host stability

---

# ğŸ§© Task 4 â€“ Monitor Container Resource Usage

---

## ğŸ“Š View Container Stats

```bash
docker stats --no-stream
```

### Observed Columns:

- CPU %
- MEM USAGE / LIMIT
- MEM %
- NET I/O
- BLOCK I/O
- PIDS

Example:

```
bridgeweb   12.14MiB / 2.899GiB   0.41%
hostweb     14.87MiB / 2.899GiB   0.50%
```

âœ” Real-time resource monitoring
âœ” Verified memory constraints applied

---

## ğŸ§  Concepts Learned

- Monitoring runtime metrics
- Memory vs CPU usage
- Network I/O tracking
- Block I/O statistics
- Container process count (PIDS)

---

# ğŸ§  Overall Concepts Learned in Secure Containers

- Immutable container design
- Filesystem protection
- Temporary writable layers
- Memory isolation
- Resource governance
- Runtime monitoring
- Basic container hardening

---

# ğŸ“¸ Snapshots

Below are screenshots demonstrating container security and monitoring.

---

## ğŸ–¼ Read-Only Filesystem Test

![Secure-1](snapshots/Secure%20Docker%20Containers/1.png)

---

## ğŸ–¼ tmpfs Writable Test

![Secure-2](snapshots/Secure%20Docker%20Containers/2.png)

---

## ğŸ–¼ Docker Stats Monitoring

![Secure-3](snapshots/Secure%20Docker%20Containers/3.png)

---

# âœ… Outcome

Successfully implemented:

âœ” Read-only container filesystem
âœ” Temporary in-memory writable directory
âœ” Memory resource limitation
âœ” Runtime container monitoring

This demonstrates foundational **Docker security best practices** required for production-grade deployments.

---
